{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Visualization\n",
    "\n",
    "This notebook demonstrates how to explore and visualize cryptocurrency market data using the BTB data loading utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from btb.data.loader import DataLoader\n",
    "from btb.data.preprocessing import DataPreprocessor\n",
    "from btb.utils.config import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load backtest configuration\n",
    "config = load_config(\"../config/backtest_config.yaml\")\n",
    "print(\n",
    "    f\"Configuration loaded for {config['backtest']['symbols'][0]} \\\n",
    "    with {config['backtest']['timeframes'][0]} timeframe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "# The DataLoader will use parameters from the load_data call below\n",
    "# or configuration if passed during initialization (e.g., API keys).\n",
    "data_loader = DataLoader() # Initialize without forcing dummy data\n",
    "\n",
    "# Load historical market data\n",
    "start_date = config[\"backtest\"][\"start_date\"]\n",
    "end_date = config[\"backtest\"][\"end_date\"]\n",
    "symbol = config[\"backtest\"][\"symbols\"][0]\n",
    "timeframe = config[\"backtest\"][\"timeframes\"][0]\n",
    "\n",
    "# Load data\n",
    "data = data_loader.load_data(symbols=[symbol], timeframes=[timeframe], start_date=start_date, end_date=end_date)\n",
    "\n",
    "# Get the DataFrame for the specific symbol and timeframe\n",
    "df = data[f\"{symbol}_{timeframe}\"]\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Statistics and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot price history\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df.index, df[\"close\"], label=\"Close Price\")\n",
    "plt.title(f\"{symbol} Price History\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Add technical indicators\n",
    "df_with_features = preprocessor.add_technical_indicators(df)\n",
    "\n",
    "# Display columns after adding indicators\n",
    "print(\"Generated features:\", df_with_features.columns.tolist())\n",
    "df_with_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset period for better visualization\n",
    "subset_period = df_with_features[-100:].copy()\n",
    "\n",
    "# Plot price with MA (using calculated indicators: ma_21, ma_50)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(subset_period.index, subset_period[\"close\"], label=\"Close Price\")\n",
    "if 'ma_21' in subset_period.columns:\n",
    "    plt.plot(subset_period.index, subset_period[\"ma_21\"], label=\"MA 21\")\n",
    "if 'ma_50' in subset_period.columns:\n",
    "    plt.plot(subset_period.index, subset_period[\"ma_50\"], label=\"MA 50\")\n",
    "plt.title(f\"{symbol} Price with Moving Averages\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix of features\n",
    "correlation = df_with_features.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "mask = np.triu(correlation)\n",
    "sns.heatmap(correlation, mask=mask, annot=False, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "df_with_features[\"daily_return\"] = df_with_features[\"close\"].pct_change()\n",
    "\n",
    "# Calculate mean and standard deviation of returns\n",
    "mean_return = df_with_features[\"daily_return\"].mean()\n",
    "std_return = df_with_features[\"daily_return\"].std()\n",
    "\n",
    "# Define outliers (3 standard deviations from mean)\n",
    "outliers = df_with_features[abs(df_with_features[\"daily_return\"] - mean_return) > 3 * std_return]\n",
    "\n",
    "# Plot returns with outliers highlighted\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_with_features.index, df_with_features[\"daily_return\"], label=\"Daily Returns\", alpha=0.5)\n",
    "plt.scatter(outliers.index, outliers[\"daily_return\"], color=\"red\", label=\"Anomalies (>3σ)\", alpha=1)\n",
    "plt.axhline(y=mean_return, color=\"g\", linestyle=\"-\", alpha=0.3, label=\"Mean Return\")\n",
    "plt.axhline(y=mean_return + 3 * std_return, color=\"r\", linestyle=\"--\", alpha=0.3, label=\"Upper Bound (3σ)\")\n",
    "plt.axhline(y=mean_return - 3 * std_return, color=\"r\", linestyle=\"--\", alpha=0.3, label=\"Lower Bound (3σ)\")\n",
    "plt.title(\"Daily Returns with Anomaly Detection\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print anomalies\n",
    "print(f\"Found {len(outliers)} anomalies in the price data\")\n",
    "if len(outliers) > 0:\n",
    "    print(outliers[[\"close\", \"daily_return\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define target (next day's return)\n",
    "df_with_features[\"target\"] = df_with_features[\"close\"].pct_change(1).shift(-1)\n",
    "\n",
    "# Drop NaN values\n",
    "df_clean = df_with_features.dropna()\n",
    "\n",
    "# Select features (exclude price data and target)\n",
    "feature_cols = [\n",
    "    col for col in df_clean.columns if col not in [\"open\", \"high\", \"low\", \"close\", \"volume\", \"target\", \"daily_return\"]\n",
    "]\n",
    "\n",
    "# Prepare data\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean[\"target\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\"Feature\": feature_cols, \"Importance\": model.feature_importances_}).sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance[:15])\n",
    "plt.title(\"Top 15 Most Important Features for Price Prediction\")\n",
    "plt.grid(True, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data for ML models using the public process method\n",
    "# Note: The process method expects a dictionary and returns a dictionary.\n",
    "# We'll wrap our DataFrame, process it, and then extract it.\n",
    "data_to_process = {f\"{symbol}_{timeframe}\": df_with_features.copy()}\n",
    "processed_data_dict = preprocessor.process(\n",
    "    data_to_process, \n",
    "    add_technical_indicators=False, # Indicators already added\n",
    "    normalize=\"min_max\",          # Specify normalization method\n",
    "    fill_missing=None             # Missing values already handled\n",
    ")\n",
    "\n",
    "# Extract the processed DataFrame\n",
    "df_normalized = processed_data_dict[f\"{symbol}_{timeframe}\"]\n",
    "\n",
    "# Select only the feature columns and the original 'close' column for saving\n",
    "# (The target was calculated earlier and might not be present after processing if NaNs were dropped)\n",
    "# Re-calculate target on the potentially shorter df_normalized if needed, or handle in model training\n",
    "df_to_save = df_normalized[feature_cols + [\"close\"]].copy()\n",
    "\n",
    "# Display normalized data\n",
    "df_to_save.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for model training\n",
    "# Note: The target column is not saved here; it should be generated during model training sequence creation.\n",
    "processed_data_path = \"../data/processed/\"\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "df_to_save.to_csv(f\"{processed_data_path}{symbol}_{timeframe}_processed.csv\")\n",
    "print(f\"Processed data saved to: {processed_data_path}{symbol}_{timeframe}_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "1. Loaded historical market data using the BTB DataLoader\n",
    "2. Visualized price history and patterns\n",
    "3. Generated technical indicators using the DataPreprocessor\n",
    "4. Performed correlation analysis to understand feature relationships\n",
    "5. Detected anomalies in the price data\n",
    "6. Analyzed feature importance for price prediction\n",
    "7. Prepared and saved normalized data for model training\n",
    "\n",
    "Next steps:\n",
    "- Use the processed data to train ML models in the model development notebook\n",
    "- Test different feature combinations for improved model performance\n",
    "- Experiment with different lookback windows and prediction horizons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
