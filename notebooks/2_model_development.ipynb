{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development and Experimentation\n",
    "\n",
    "This notebook demonstrates how to develop, train, and evaluate machine learning models for predicting cryptocurrency price movements using BTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: bad revision 'HEAD'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# import torch.optim as optim # Removed, handled by model classes\n",
    "# from torch.utils.data import DataLoader, TensorDataset # Removed, handled by model classes\n",
    "# from sklearn.preprocessing import StandardScaler # Removed, handled by DataPreprocessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from btb.data.preprocessing import DataPreprocessor  # Added\n",
    "from btb.models.lstm import LSTMModel\n",
    "from btb.models.transformer import TransformerModel\n",
    "from btb.utils.config import load_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Load model configuration\n",
    "config = load_config(\"../config/model_config.yaml\")\n",
    "print(\"Model configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from: ../data/processed/BTCUSDT_1h_processed.csv\n",
      "Processed data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load backtest config to get symbol/timeframe used for processed data\n",
    "backtest_config = load_config(\"../config/backtest_config.yaml\")\n",
    "symbol = backtest_config[\"backtest\"][\"symbols\"][0]\n",
    "timeframe = backtest_config[\"backtest\"][\"timeframes\"][0]\n",
    "\n",
    "# Construct path and load the processed data\n",
    "processed_data_path = f\"../data/processed/{symbol}_{timeframe}_processed.csv\"\n",
    "print(f\"Loading processed data from: {processed_data_path}\")\n",
    "try:\n",
    "    df = pd.read_csv(processed_data_path, index_col=0, parse_dates=True)\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    df.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Processed data file not found. Please run notebook 1 first.\")\n",
    "    # Optionally raise error or exit\n",
    "    # raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Time Series Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequences - X shape: (8478, 60, 22), y shape: (8478,)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate DataPreprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Set sequence length and prediction horizon from model config\n",
    "seq_length = config[\"features\"][\"sequence_length\"]\n",
    "prediction_horizon = config[\"prediction\"].get(\"prediction_horizon\", 1)\n",
    "target_column = \"close\"  # Default target for sequence creation\n",
    "classification_mode = config[\"prediction\"].get(\"output_type\", \"regression\") == \"classification\"\n",
    "classification_threshold = config[\"prediction\"].get(\n",
    "    \"confidence_threshold\", 0.0\n",
    ")  # Use confidence_threshold if classification\n",
    "\n",
    "# Create sequences using DataPreprocessor\n",
    "# Note: Ensure the input df has the necessary columns (features + target_column)\n",
    "# The preprocessor handles target creation internally based on target_column and horizon.\n",
    "X, y = preprocessor.create_sequences(\n",
    "    data=df,\n",
    "    sequence_length=seq_length,\n",
    "    target_column=target_column,\n",
    "    prediction_horizon=prediction_horizon,\n",
    "    classification=classification_mode,\n",
    "    threshold=classification_threshold,\n",
    ")\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Generated sequences - X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (5934, 60, 22), Validation: (1271, 60, 22), Test: (1273, 60, 22)\n"
     ]
    }
   ],
   "source": [
    "# Split into train, validation, test sets\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size : train_size + val_size], y[train_size : train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size :], y[train_size + val_size :]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data converted to PyTorch tensors.\n"
     ]
    }
   ],
   "source": [
    "# Convert split data to PyTorch tensors\n",
    "# The model's train method expects tuples of (X, y) tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val).view(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "print(\"Data converted to PyTorch tensors.\")\n",
    "# Removed DataLoader creation - handled internally by model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Evaluate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LSTMModel...\n",
      "LSTMModel initialized on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Prepare LSTM model configuration from the loaded model_config.yaml\n",
    "# The LSTMModel class expects specific keys in its config dict\n",
    "lstm_model_config = config.get(\"model\", {}).copy()  # Get model specific params\n",
    "lstm_training_config = config.get(\"training\", {}).copy()  # Get training params\n",
    "\n",
    "# Combine and adapt parameters for LSTMModel constructor\n",
    "lstm_combined_config = {\n",
    "    \"input_dim\": X_train_tensor.shape[2],  # Number of features in input sequences\n",
    "    \"output_dim\": 1,  # Predicting a single value\n",
    "    \"hidden_dim\": lstm_model_config.get(\"hidden_dim\", 128),\n",
    "    \"num_layers\": lstm_model_config.get(\"num_layers\", 2),\n",
    "    \"dropout\": float(lstm_model_config.get(\"dropout\", 0.1)),  # Ensure float\n",
    "    \"learning_rate\": float(lstm_training_config.get(\"learning_rate\", 0.001)),  # Ensure float\n",
    "    \"epochs\": lstm_training_config.get(\"epochs\", 10),\n",
    "    \"batch_size\": lstm_training_config.get(\"batch_size\", 64),\n",
    "    \"patience\": lstm_training_config.get(\"patience\", 15),\n",
    "    \"early_stopping\": lstm_training_config.get(\"early_stopping\", True),\n",
    "    \"optimizer\": lstm_training_config.get(\"optimizer\", \"adam\"),\n",
    "    \"scheduler\": lstm_training_config.get(\"scheduler\"),\n",
    "    \"weight_decay\": float(lstm_training_config.get(\"weight_decay\", 0)),  # Ensure float\n",
    "    \"gradient_clipping\": lstm_training_config.get(\"gradient_clipping\"),\n",
    "}\n",
    "\n",
    "# Initialize LSTM model using the class from btb.models\n",
    "print(\"Initializing LSTMModel...\")\n",
    "lstm_model = LSTMModel(config=lstm_combined_config)\n",
    "print(f\"LSTMModel initialized on device: {lstm_model.device}\")\n",
    "# Optimizer and criterion are handled internally by the LSTMModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model using its train method\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_train_metrics = lstm_model.train(\n",
    "    train_data=(X_train_tensor, y_train_tensor), validation_data=(X_val_tensor, y_val_tensor)\n",
    ")\n",
    "print(\"LSTM training complete.\")\n",
    "print(f\"Final LSTM Train Loss: {lstm_train_metrics.get('final_train_loss', 'N/A'):.6f}\")\n",
    "print(f\"Final LSTM Val Loss: {lstm_train_metrics.get('final_val_loss', 'N/A'):.6f}\")\n",
    "print(f\"Best LSTM Val Loss: {lstm_train_metrics.get('best_val_loss', 'N/A'):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss from metrics dictionary\n",
    "plt.figure(figsize=(10, 6))\n",
    "if \"train_losses\" in lstm_train_metrics:\n",
    "    plt.plot(lstm_train_metrics[\"train_losses\"], label=\"Train Loss\")\n",
    "if \"val_losses\" in lstm_train_metrics:\n",
    "    plt.plot(lstm_train_metrics[\"val_losses\"], label=\"Validation Loss\")\n",
    "plt.title(\"LSTM Model Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "if \"train_losses\" in lstm_train_metrics or \"val_losses\" in lstm_train_metrics:\n",
    "    plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Transformer model configuration from the loaded model_config.yaml\n",
    "transformer_model_config = config.get(\"model\", {}).copy()\n",
    "transformer_training_config = config.get(\"training\", {}).copy()\n",
    "\n",
    "# Combine and adapt parameters for TransformerModel constructor\n",
    "transformer_combined_config = {\n",
    "    \"feature_dim\": X_train_tensor.shape[2],  # Number of features\n",
    "    \"output_dim\": 1,  # Predicting a single value\n",
    "    \"d_model\": transformer_model_config.get(\"hidden_dim\", 128),  # Use hidden_dim as d_model\n",
    "    \"nhead\": transformer_model_config.get(\"attention_heads\", 8),\n",
    "    \"num_encoder_layers\": transformer_model_config.get(\"num_layers\", 3),\n",
    "    \"dim_feedforward\": transformer_model_config.get(\"dim_feedforward\", 2048),  # Add default if missing\n",
    "    \"dropout\": float(transformer_model_config.get(\"dropout\", 0.1)),  # Ensure float\n",
    "    \"learning_rate\": float(transformer_training_config.get(\"learning_rate\", 0.001)),  # Ensure float\n",
    "    \"epochs\": transformer_training_config.get(\"epochs\", 10),\n",
    "    \"batch_size\": transformer_training_config.get(\"batch_size\", 64),\n",
    "    \"patience\": transformer_training_config.get(\"patience\", 15),\n",
    "    \"early_stopping\": transformer_training_config.get(\"early_stopping\", True),\n",
    "    \"optimizer\": transformer_training_config.get(\"optimizer\", \"adam\"),\n",
    "    \"scheduler\": transformer_training_config.get(\"scheduler\"),\n",
    "    \"weight_decay\": float(transformer_training_config.get(\"weight_decay\", 0)),  # Ensure float\n",
    "    \"gradient_clipping\": transformer_training_config.get(\"gradient_clipping\"),\n",
    "}\n",
    "\n",
    "# Initialize Transformer model using the class from btb.models\n",
    "print(\"Initializing TransformerModel...\")\n",
    "transformer_model = TransformerModel(config=transformer_combined_config)\n",
    "print(f\"TransformerModel initialized on device: {transformer_model.device}\")\n",
    "# Optimizer and criterion are handled internally by the TransformerModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer model using its train method\n",
    "print(\"Training Transformer model...\")\n",
    "transformer_train_metrics = transformer_model.train(\n",
    "    train_data=(X_train_tensor, y_train_tensor), validation_data=(X_val_tensor, y_val_tensor)\n",
    ")\n",
    "print(\"Transformer training complete.\")\n",
    "print(f\"Final Transformer Train Loss: {transformer_train_metrics.get('final_train_loss', 'N/A'):.6f}\")\n",
    "print(f\"Final Transformer Val Loss: {transformer_train_metrics.get('final_val_loss', 'N/A'):.6f}\")\n",
    "print(f\"Best Transformer Val Loss: {transformer_train_metrics.get('best_val_loss', 'N/A'):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss from metrics dictionary\n",
    "plt.figure(figsize=(10, 6))\n",
    "if \"train_losses\" in transformer_train_metrics:\n",
    "    plt.plot(transformer_train_metrics[\"train_losses\"], label=\"Train Loss\")\n",
    "if \"val_losses\" in transformer_train_metrics:\n",
    "    plt.plot(transformer_train_metrics[\"val_losses\"], label=\"Validation Loss\")\n",
    "plt.title(\"Transformer Model Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "if \"train_losses\" in transformer_train_metrics or \"val_losses\" in transformer_train_metrics:\n",
    "    plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model using its predict method\n",
    "print(\"Evaluating LSTM model...\")\n",
    "lstm_predictions = lstm_model.predict(X_test_tensor)\n",
    "lstm_actuals = y_test_tensor.cpu().numpy().flatten()\n",
    "lstm_predictions = lstm_predictions.flatten()\n",
    "\n",
    "# Calculate LSTM metrics\n",
    "lstm_mse = mean_squared_error(lstm_actuals, lstm_predictions)\n",
    "lstm_rmse = np.sqrt(lstm_mse)\n",
    "lstm_mae = mean_absolute_error(lstm_actuals, lstm_predictions)\n",
    "lstm_r2 = r2_score(lstm_actuals, lstm_predictions)\n",
    "lstm_results = {\n",
    "    \"predictions\": lstm_predictions,\n",
    "    \"actuals\": lstm_actuals,\n",
    "    \"mse\": lstm_mse,\n",
    "    \"rmse\": lstm_rmse,\n",
    "    \"mae\": lstm_mae,\n",
    "    \"r2\": lstm_r2,\n",
    "}\n",
    "\n",
    "# Evaluate Transformer model using its predict method\n",
    "print(\"Evaluating Transformer model...\")\n",
    "transformer_predictions = transformer_model.predict(X_test_tensor)\n",
    "transformer_actuals = y_test_tensor.cpu().numpy().flatten()  # Same actuals as LSTM\n",
    "transformer_predictions = transformer_predictions.flatten()\n",
    "\n",
    "# Calculate Transformer metrics\n",
    "transformer_mse = mean_squared_error(transformer_actuals, transformer_predictions)\n",
    "transformer_rmse = np.sqrt(transformer_mse)\n",
    "transformer_mae = mean_absolute_error(transformer_actuals, transformer_predictions)\n",
    "transformer_r2 = r2_score(transformer_actuals, transformer_predictions)\n",
    "transformer_results = {\n",
    "    \"predictions\": transformer_predictions,\n",
    "    \"actuals\": transformer_actuals,\n",
    "    \"mse\": transformer_mse,\n",
    "    \"rmse\": transformer_rmse,\n",
    "    \"mae\": transformer_mae,\n",
    "    \"r2\": transformer_r2,\n",
    "}\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"LSTM Model Metrics:\")\n",
    "print(f\"MSE: {lstm_results['mse']:.6f}\")\n",
    "print(f\"RMSE: {lstm_results['rmse']:.6f}\")\n",
    "print(f\"MAE: {lstm_results['mae']:.6f}\")\n",
    "print(f\"R²: {lstm_results['r2']:.6f}\")\n",
    "print(\"\\nTransformer Model Metrics:\")\n",
    "print(f\"MSE: {transformer_results['mse']:.6f}\")\n",
    "print(f\"RMSE: {transformer_results['rmse']:.6f}\")\n",
    "print(f\"MAE: {transformer_results['mae']:.6f}\")\n",
    "print(f\"R²: {transformer_results['r2']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot a subset for better visualization\n",
    "subset_size = min(300, len(lstm_results[\"actuals\"]))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(lstm_results[\"actuals\"][:subset_size], label=\"Actual\")\n",
    "plt.plot(lstm_results[\"predictions\"][:subset_size], label=\"LSTM Predictions\")\n",
    "plt.title(\"LSTM Model: Predictions vs Actuals\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(transformer_results[\"actuals\"][:subset_size], label=\"Actual\")\n",
    "plt.plot(transformer_results[\"predictions\"][:subset_size], label=\"Transformer Predictions\")\n",
    "plt.title(\"Transformer Model: Predictions vs Actuals\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction errors\n",
    "lstm_errors = lstm_results[\"predictions\"] - lstm_results[\"actuals\"]\n",
    "transformer_errors = transformer_results[\"predictions\"] - transformer_results[\"actuals\"]\n",
    "\n",
    "# Plot error distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(lstm_errors, bins=50, alpha=0.7)\n",
    "plt.title(\"LSTM Error Distribution\")\n",
    "plt.axvline(x=0, color=\"r\", linestyle=\"--\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(transformer_errors, bins=50, alpha=0.7)\n",
    "plt.title(\"Transformer Error Distribution\")\n",
    "plt.axvline(x=0, color=\"r\", linestyle=\"--\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of simple hyperparameter tuning for LSTM\n",
    "# In practice, you would use a more systematic approach (e.g., grid search, random search, Bayesian optimization)\n",
    "# This function now uses the LSTMModel class from btb.models\n",
    "\n",
    "\n",
    "def tune_lstm_hyperparameters(hidden_dims, num_layers_options, dropout_rates, base_config, train_data, val_data):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    # Get base parameters needed for model init\n",
    "    input_dim = train_data[0].shape[2]\n",
    "    output_dim = 1\n",
    "    num_test_epochs = 5  # Reduced epochs for tuning\n",
    "\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for num_layers in num_layers_options:\n",
    "            for dropout in dropout_rates:\n",
    "                print(f\"Tuning LSTM: hidden={hidden_dim}, layers={num_layers}, dropout={dropout}\")\n",
    "                # Create a temporary config for this run\n",
    "                temp_config = base_config.copy()  # Start with base LSTM params\n",
    "                temp_config[\"input_dim\"] = input_dim\n",
    "                temp_config[\"output_dim\"] = output_dim\n",
    "                temp_config[\"hidden_dim\"] = hidden_dim\n",
    "                temp_config[\"num_layers\"] = num_layers\n",
    "                temp_config[\"dropout\"] = float(dropout)  # Ensure float\n",
    "                temp_config[\"epochs\"] = num_test_epochs  # Use reduced epochs\n",
    "\n",
    "                # Initialize model with current hyperparameters\n",
    "                model = LSTMModel(config=temp_config)\n",
    "\n",
    "                # Train using the model's train method\n",
    "                train_metrics = model.train(train_data=train_data, validation_data=val_data)\n",
    "\n",
    "                # Record results (use best_val_loss from metrics if available)\n",
    "                final_val_loss = train_metrics.get(\"best_val_loss\", train_metrics.get(\"final_val_loss\", float(\"inf\")))\n",
    "                results.append(\n",
    "                    {\"hidden_dim\": hidden_dim, \"num_layers\": num_layers, \"dropout\": dropout, \"val_loss\": final_val_loss}\n",
    "                )\n",
    "\n",
    "                # Track best model\n",
    "                if final_val_loss < best_val_loss:\n",
    "                    best_val_loss = final_val_loss\n",
    "                    best_params = {\"hidden_dim\": hidden_dim, \"num_layers\": num_layers, \"dropout\": dropout}\n",
    "                    print(f\"  -> New best val_loss: {best_val_loss:.6f}\")\n",
    "\n",
    "    return pd.DataFrame(results), best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space\n",
    "hidden_dims = [64, 128]\n",
    "num_layers_options = [1, 2]\n",
    "dropout_rates = [0.1, 0.3]\n",
    "\n",
    "# Prepare data tuples for tuning function\n",
    "train_data_tuple = (X_train_tensor, y_train_tensor)\n",
    "val_data_tuple = (X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Run hyperparameter tuning (uncomment to run - this can take time)\n",
    "# tuning_results, best_params = tune_lstm_hyperparameters(\n",
    "#     hidden_dims, num_layers_options, dropout_rates,\n",
    "#     lstm_combined_config, # Pass the base LSTM config used for main training\n",
    "#     train_data_tuple, val_data_tuple\n",
    "# )\n",
    "#\n",
    "# print(\"Hyperparameter tuning results:\")\n",
    "# print(tuning_results.sort_values('val_loss'))\n",
    "# print(\"\\nBest parameters:\")\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trade Signal Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trading_signals(predictions, threshold=0.0):\n",
    "    \"\"\"Generate trading signals based on predictions\"\"\"\n",
    "    signals = np.zeros_like(predictions)\n",
    "    signals[predictions > threshold] = 1  # Buy signal\n",
    "    signals[predictions < -threshold] = -1  # Sell signal\n",
    "    return signals\n",
    "\n",
    "\n",
    "# Generate signals\n",
    "lstm_signals = generate_trading_signals(lstm_results[\"predictions\"], threshold=0.001)\n",
    "transformer_signals = generate_trading_signals(transformer_results[\"predictions\"], threshold=0.001)\n",
    "\n",
    "# Count signal types\n",
    "print(\"LSTM Signals:\")\n",
    "print(f\"Buy signals: {np.sum(lstm_signals == 1)}\")\n",
    "print(f\"Sell signals: {np.sum(lstm_signals == -1)}\")\n",
    "print(f\"Hold signals: {np.sum(lstm_signals == 0)}\")\n",
    "\n",
    "print(\"\\nTransformer Signals:\")\n",
    "print(f\"Buy signals: {np.sum(transformer_signals == 1)}\")\n",
    "print(f\"Sell signals: {np.sum(transformer_signals == -1)}\")\n",
    "print(f\"Hold signals: {np.sum(transformer_signals == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = \"../models/\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save models using their save method\n",
    "lstm_model_path = f\"{models_dir}lstm_model.pth\"\n",
    "transformer_model_path = f\"{models_dir}transformer_model.pth\"\n",
    "\n",
    "lstm_model.save(lstm_model_path)\n",
    "transformer_model.save(transformer_model_path)\n",
    "\n",
    "print(f\"LSTM model saved to {lstm_model_path}\")\n",
    "print(f\"Transformer model saved to {transformer_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. Loaded and prepared time series data for ML modeling\n",
    "2. Implemented and trained LSTM and Transformer models for price prediction\n",
    "3. Evaluated and compared model performance using various metrics\n",
    "4. Explored error distributions and prediction patterns\n",
    "5. Demonstrated basic hyperparameter tuning\n",
    "6. Generated trading signals based on model predictions\n",
    "7. Saved trained models for use in backtesting and live trading\n",
    "\n",
    "Next steps:\n",
    "- Use these models in the backtesting framework to evaluate trading performance\n",
    "- Refine model architectures and hyperparameters for better performance\n",
    "- Explore ensemble methods for more robust predictions\n",
    "- Implement advanced training techniques (e.g., focal loss, adversarial training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
